# -*- coding: utf-8 -*-
"""ComputerVisionProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kOO5TA57IqzlSHPZ0A9xPEXNowZ5Va-H
"""

import os
import glob
import sklearn
import numpy as np
import pandas as pd
import seaborn as sns
import urllib.request
import tensorflow as tf
from PIL import Image as im
from matplotlib import pyplot
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

"""Importing images of following classes"""

classes = ['cat','bicycle', 'bear','airplane',
                'ant','banana','bench','book',
                'bottlecap', 'bread']

url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'

"""Making combined image dataset of all classes"""

# Download the data of the aforementioned classes
for class_ in classes:
	complete_url = url+class_+".npy"
	print("Downloading : ",complete_url)
	urllib.request.urlretrieve(complete_url, "./"+class_+".npy")

# Grep all the downloaded files and add them to a list
data_sets = glob.glob(os.path.join('./*.npy'))

"""Creating train and test datasets"""

#initialize variables
# Train data
input = np.empty([0, 784])
# Test data
labels = np.empty([0])
index = 0

# Concatenate the train and test data from all the files
for file in data_sets:
	data = np.load(file)
	data = data[0: 6000, :]
	input = np.concatenate((input, data), axis=0)
	labels = np.append(labels, [index]*data.shape[0])
	index += 1

# K-Folds cross-validator
n_fold = 5
kf = KFold(n_splits=n_fold, shuffle=True, random_state=9)
x_train, x_test, y_train, y_test = None, None, None, None
random_ordering = np.random.permutation(input.shape[0])
input = input[random_ordering, :]
labels = labels[random_ordering]

# Divide the dataset into train and test
for train_index, test_index in kf.split(input):
    x_train, x_test = input[train_index], input[test_index]
    y_train, y_test = labels[train_index], labels[test_index]
    break

print(len(x_train))
print(len(x_test))

print(x_train[:20])
print(x_test[:20])
print(y_train[:20])
print(y_test[:20])

"""Visualising the input images"""

import matplotlib.pyplot as plt
image_size = 28
x_train_ = x_train.reshape(x_train.shape[0], image_size, image_size)
for i in range(16):
  plt.grid(False)
  plt.imshow(x_train_[i], cmap=plt.cm.binary)
  plt.show()

"""Normalisation"""

# Divide all the values by 255 to normalize the image
x_train /= 255.0
x_test /= 255.0

# Saving for feature selection and 1d classifiers
x_train_fea = x_train
x_test_fea = x_test
x_train_1d = x_train
x_test_1d = x_test

"""Reshaping"""

# Reshape the image size to be 28 x 28
image_size = 28
x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1)
x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1)

num_classes = len(classes)

"""# Random Forest"""

from sklearn.model_selection import GridSearchCV
#from sklearn.ensemble.forest import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
parameters = {'n_estimators': [100,120,140,160]}

clf_rf = RandomForestClassifier(n_estimators = 150, n_jobs=-1, random_state=0)
# model = GridSearchCV(clf_rf, parameters, n_jobs=-1)
clf_rf.fit(x_train_1d, y_train)

"""Predicting"""

y_pred = clf_rf.predict(x_test_1d)
y_pred[:10]

"""Checkng Test Accuracy"""

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

tuning_parameters = {'n_neighbors': [1,3,5,7,9,11]}

knn = KNeighborsClassifier(n_neighbors = 5, n_jobs=-1)
# knn = GridSearchCV(clf_knn, tuning_parameters, n_jobs=-1)
knn.fit(x_train_1d, y_train)

"""Predicting"""

y_pred = knn.predict(x_test_1d)
y_pred[:10]

"""Testing Accuracy"""

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

"""# MLP"""

from sklearn.neural_network import MLPClassifier

tuning_parameaters = {'hidden_layer_sizes' : [(50,), (100,), (784,), (50,50), (100,100), (784,784), (50,50,50), (100,100,100)],
                    'alpha' : list(10.0 ** -np.arange(1, 7))}

mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), random_state=0)
# mlp = GridSearchCV(clf_mlp, param_grid=tuning_parameaters, n_jobs=-1)
mlp.fit(x_train_1d, y_train)

"""Predicting"""

y_pred = mlp.predict(x_test_1d)
y_pred[:10]

"""Testing Accuracy"""

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

"""# CNN"""

# CNN Model
model = keras.Sequential()
model.add(layers.Convolution2D(64, (3, 3),
                        padding='same',
                        input_shape=x_train.shape[1:], activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(3, 3)))
model.add(layers.Convolution2D(128, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(3, 3)))
model.add(layers.Convolution2D(64, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D(pool_size =(3,3)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(num_classes, activation='softmax'))
optimizer = tf.optimizers.Adam()
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

"""Fitting model"""

# Fit a model to the train data
model.fit(x = x_train, y = y_train, batch_size = 100,  validation_split = 0.2, epochs=5)

"""Predicting"""

y_predict = model.predict(x_test)
y_predict[:2]

"""Testing Accuracy"""

# Obtain the accuracy of the above model on the test data
accuracy = model.evaluate(x_test, y_test)

"""# Feature Selection"""

# Taking old data inputs
x_train = x_train_fea
x_test = x_test_fea

x_train_1d = x_train_fea
x_test_1d = x_test_fea
from sklearn.feature_selection import chi2, SelectKBest
n_features = 400
bestfeatures = SelectKBest(score_func=chi2, k=100)
fit = bestfeatures.fit(x_train_1d, y_train)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(range(len(x_train_1d)))
#concat two dataframes for better visualization

featureScores = pd.concat([dfcolumns, dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
print(featureScores.nlargest(n_features,'Score'))  #print 1 best features

fit.scores_[100:120]

featureScores.nlargest(n_features,'Score')

sns.displot(data=featureScores.nlargest(n_features,'Score'), x="Score", y="Specs", color="Red")

train_x_new = []
for x_tr in x_train_1d:
  temp = []
  for i in featureScores.nlargest(n_features, 'Score')['Specs']:
      temp.append(x_tr[i])
  train_x_new.append(temp)

train_x_new[0][115:120]

test_x_new = []
for x_te in x_test_1d:
  temp = []
  for i in featureScores.nlargest(n_features, 'Score')['Specs']:
      temp.append(x_te[i])
  test_x_new.append(temp)

test_x_new[0][:10]

print(len(train_x_new))
print(len(test_x_new))

"""# Updating the dataset with new features"""

x_train = np.array(train_x_new)
x_test = np.array(test_x_new)

x_train_1d = x_train
x_test_1d = x_test
# Reshape the image size to be 20 x 20
image_size = 20
x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1)
x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1)

"""# Random Forest"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
parameters = {'n_estimators': [100,120,140,160]}

clf_rf_fe = RandomForestClassifier(n_estimators = 280, n_jobs=-1, random_state=0)
# clf_rf = GridSearchCV(clf_rf, parameters, n_jobs=-1)
clf_rf_fe.fit(x_train_1d, y_train)

y_pred = clf_rf_fe.predict(x_test_1d)
print(y_pred[:10])

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

tuning_parameters = {'n_neighbors': [1,3,5,7,9,11]}

knn_fe = KNeighborsClassifier(n_neighbors = 5, n_jobs=-1)
# knn = GridSearchCV(clf_knn, tuning_parameters, n_jobs=-1)
knn_fe.fit(x_train_1d, y_train)

y_pred = knn_fe.predict(x_test_1d)
print(y_pred[:10])

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

"""# MLP"""

from sklearn.neural_network import MLPClassifier

tuning_parameaters = {'hidden_layer_sizes' : [(50,), (100,), (784,), (50,50), (100,100), (784,784), (50,50,50), (100,100,100)],
                    'alpha' : list(10.0 ** -np.arange(1, 7))}

mlp_fe = MLPClassifier(hidden_layer_sizes=(784,784), random_state=0)
# mlp = GridSearchCV(clf_mlp, param_grid=tuning_parameaters, n_jobs=-1)
mlp_fe.fit(x_train_1d, y_train)

y_pred = mlp_fe.predict(x_test_1d)
print(y_pred[:10])

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

"""# CNN"""

# Reshaping
image_size = 28
x_train = x_train_fea.reshape(x_train.shape[0], image_size, image_size, 1)
x_test = x_test_fea.reshape(x_test.shape[0], image_size, image_size, 1)

# CNN Model
model_fe = keras.Sequential()
model_fe.add(layers.Convolution2D(64, (3, 3),
                        padding='same',
                        input_shape=x_train.shape[1:], activation='relu'))
model_fe.add(layers.MaxPooling2D(pool_size=(3, 3)))
model_fe.add(layers.Convolution2D(128, (3, 3), padding='same', activation='relu'))
model_fe.add(layers.MaxPooling2D(pool_size=(3, 3)))
model_fe.add(layers.Convolution2D(64, (3, 3), padding='same', activation='relu'))
model_fe.add(layers.MaxPooling2D(pool_size =(3,3)))
model_fe.add(layers.Flatten())
model_fe.add(layers.Dense(128, activation='relu'))
model_fe.add(layers.Dense(len(classes), activation='softmax'))
optimizer = tf.optimizers.Adam()
model_fe.compile(loss='sparse_categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

# Fit a model to the train data
model_fe.fit(x = x_train, y = y_train, batch_size = 100,  validation_split = 0.2, epochs=15)

y_pred = model_fe.predict(x_test)
print(y_pred[:2])

# Obtain the accuracy of the above model on the test data
accuracy = model_fe.evaluate(x_test, y_test)
print("Accuracy =", accuracy)

df = pd.DataFrame([[0.769, 0.788]])
ax = sns.barplot(data=df, ci="sd")

df = pd.DataFrame([[0.707, 0.73375]])
ax = sns.barplot(data=df, ci="sd")

df = pd.DataFrame([[0.759, 0.788]])
ax = sns.barplot(data=df, ci="sd")

df = pd.DataFrame([[0.8717, 0.8924]])
ax = sns.barplot(data=df, ci="sd")